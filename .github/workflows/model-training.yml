name: MLflow CI/CD Model Training

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'MLProject/**'
      - '.github/workflows/**'
      - '*.py'
      - '*.yaml'
      - '*.yml'
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      experiment_name:
        description: 'MLflow experiment name'
        required: false
        default: 'iris_classification_ci_alpian_khairi'
      n_estimators:
        description: 'Number of estimators'
        required: false
        default: '100'
      max_depth:
        description: 'Maximum depth'
        required: false
        default: '10'

jobs:
  model-training:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Verify conda.yaml exists
      run: |
        echo "Checking for conda.yaml file..."
        if [ -f conda.yaml ]; then
          echo "✅ conda.yaml found!"
          echo "conda.yaml content:"
          cat conda.yaml
        else
          echo "❌ conda.yaml not found in root directory!"
          echo "Files in root directory:"
          ls -la
          exit 1
        fi
      
    - name: Debug - List project structure
      run: |
        echo "Root directory contents:"
        ls -la
        echo "MLProject directory contents (if exists):"
        ls -la MLProject/ || echo "MLProject directory not found"
        echo "Looking for conda.yaml and MLProject files..."
        find . -name "conda.yaml" -o -name "MLProject" -o -name "*.py" | head -20
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Setup Miniconda
      uses: conda-incubator/setup-miniconda@v3
      with:
        miniconda-version: "latest"
        activate-environment: iris-classification-env
        environment-file: ./conda.yaml
        python-version: 3.9
        auto-activate-base: false
        auto-update-conda: true
      continue-on-error: true
      
    - name: Setup Python environment (Fallback if conda fails)
      if: failure()
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies with pip (Fallback)
      if: failure()
      run: |
        pip install --upgrade pip
        pip install numpy==1.24.4 pandas==2.1.4 scikit-learn==1.3.2 matplotlib==3.8.2 seaborn==0.13.0
        pip install mlflow==2.8.1 dagshub python-dotenv joblib
        
    - name: Verify environment
      run: |
        echo "Checking Python environment..."
        which python
        python --version
        echo "Checking installed packages..."
        pip list | grep -E "(numpy|pandas|scikit-learn|mlflow|dagshub)" || echo "Some packages might be missing"
        
        # Try conda if available
        if command -v conda &> /dev/null; then
          echo "Conda is available:"
          conda info || echo "Conda info failed"
          conda list || echo "Conda list failed"
        else
          echo "Conda is not available, using pip environment"
        fi
        
    - name: Install additional dependencies
      run: |
        # Try with conda first, then pip
        if command -v conda &> /dev/null; then
          echo "Using conda environment..."
          conda activate iris-classification-env || echo "Conda activate failed"
          pip install mlflow dagshub python-dotenv
        else
          echo "Using pip environment..."
          pip install mlflow dagshub python-dotenv
        fi
        echo "Installed packages:"
        pip list | grep -E "(mlflow|dagshub|dotenv)" || echo "Some packages might not be installed"
        
    - name: Verify MLProject structure
      run: |
        echo "Checking MLProject file..."
        if [ -f MLProject ]; then
          echo "✅ MLProject file found!"
          echo "MLProject file content:"
          cat MLProject
        else
          echo "❌ MLProject file not found!"
          exit 1
        fi
        
        echo "Checking modelling.py..."
        if [ -f modelling.py ]; then
          echo "✅ modelling.py found!"
        else
          echo "❌ modelling.py not found!"
          exit 1
        fi
        
    - name: Set up environment variables
      run: |
        echo "DAGSHUB_TOKEN=${{ secrets.DAGSHUB_TOKEN }}" >> $GITHUB_ENV
        echo "MLFLOW_TRACKING_USERNAME=alvian2022" >> $GITHUB_ENV
        echo "MLFLOW_TRACKING_PASSWORD=${{ secrets.DAGSHUB_TOKEN }}" >> $GITHUB_ENV
        echo "PYTHONPATH=$GITHUB_WORKSPACE:$PYTHONPATH" >> $GITHUB_ENV
        
    - name: Prepare data file (create sample if not exists)
      run: |
        # Activate conda environment if available, otherwise use default python
        if command -v conda &> /dev/null; then
          conda activate iris-classification-env || echo "Conda activate failed, using default python"
        fi
        
        python -c "
        import pandas as pd
        from sklearn.datasets import load_iris
        import os

        # Create sample data if iris_preprocessing.csv doesn't exist
        if not os.path.exists('iris_preprocessing.csv'):
            print('Creating sample iris dataset...')
            iris = load_iris()
            df = pd.DataFrame(iris.data, columns=iris.feature_names)
            df['target'] = iris.target
            df.to_csv('iris_preprocessing.csv', index=False)
            print('Sample data created: iris_preprocessing.csv')
            print(f'Data shape: {df.shape}')
        else:
            print('iris_preprocessing.csv already exists')
        "
        
    - name: Run MLflow Project Training (Using training entry point)
      run: |
        # Activate conda environment if available
        if command -v conda &> /dev/null; then
          conda activate iris-classification-env || echo "Conda activate failed, using default python"
        fi
        
        echo "Running MLflow project with training entry point..."
        echo "Current directory: $(pwd)"
        echo "Available files:"
        ls -la
        
        # Run the MLflow project using the training entry point
        mlflow run . \
          --entry-point training \
          --experiment-name "${{ github.event.inputs.experiment_name || 'iris_classification_ci_alpian_khairi' }}" \
          -P data_path="iris_preprocessing.csv" \
          -P n_estimators=${{ github.event.inputs.n_estimators || 100 }} \
          -P max_depth=${{ github.event.inputs.max_depth || 10 }} \
          -P random_state=42 \
          --env-manager conda \
          --no-conda || echo "MLflow project run failed, trying main entry point..."
      continue-on-error: true
      
    - name: Run MLflow Project Training (Using main entry point - fallback)
      run: |
        # Activate conda environment if available
        if command -v conda &> /dev/null; then
          conda activate iris-classification-env || echo "Conda activate failed, using default python"
        fi
        
        echo "Trying main entry point as fallback..."
        
        # Run the MLflow project using the main entry point
        mlflow run . \
          --entry-point main \
          --experiment-name "${{ github.event.inputs.experiment_name || 'iris_classification_ci_alpian_khairi' }}" \
          -P data_path="iris_preprocessing.csv" \
          -P experiment_name="${{ github.event.inputs.experiment_name || 'iris_classification_ci_alpian_khairi' }}" \
          -P model_name="iris_classifier_ci" \
          --env-manager conda \
          --no-conda || echo "Main entry point also failed, trying direct Python execution..."
      continue-on-error: true
      
    - name: Run Direct Python Training (Fallback Method)
      run: |
        # Activate conda environment if available
        if command -v conda &> /dev/null; then
          conda activate iris-classification-env || echo "Conda activate failed, using default python"
        fi
        
        echo "Running direct Python training as fallback..."
        python modelling.py \
          --data_path "iris_preprocessing.csv" \
          --experiment_name "${{ github.event.inputs.experiment_name || 'iris_classification_ci_alpian_khairi' }}" \
          --n_estimators ${{ github.event.inputs.n_estimators || 100 }} \
          --max_depth ${{ github.event.inputs.max_depth || 10 }} \
          --random_state 42
          
    - name: Verify artifacts
      run: |
        echo "Generated artifacts:"
        ls -la *.pkl *.png *.txt *.md 2>/dev/null || echo "No artifacts in root"
        echo "Model artifacts directory:"
        ls -la model_artifacts/ 2>/dev/null || echo "No model_artifacts directory"
        echo "All files in current directory:"
        find . -name "*.pkl" -o -name "*.png" -o -name "*.txt" -o -name "*.md" | head -10
        
    - name: Upload training artifacts
      uses: actions/upload-artifact@v4
      with:
        name: model-artifacts-${{ github.run_number }}
        path: |
          *.pkl
          *.png
          *.txt
          *.md
          model_artifacts/
        retention-days: 30
        if-no-files-found: warn
        
    - name: Create workflow summary
      run: |
        echo "# Training Summary" >> $GITHUB_STEP_SUMMARY
        echo "- **Workflow Run**: #${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Timestamp**: $(date)" >> $GITHUB_STEP_SUMMARY
        echo "- **Parameters**: n_estimators=${{ github.event.inputs.n_estimators || 100 }}, max_depth=${{ github.event.inputs.max_depth || 10 }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Experiment**: ${{ github.event.inputs.experiment_name || 'iris_classification_ci_alpian_khairi' }}" >> $GITHUB_STEP_SUMMARY
        
        echo "## Generated Files" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        ls -la *.pkl *.png *.txt *.md 2>/dev/null || echo "No artifacts found"
        echo '```' >> $GITHUB_STEP_SUMMARY
        
        # Also create a simple summary file
        echo "Training completed at $(date)" > training_summary.txt
        echo "Workflow: ${{ github.run_number }}" >> training_summary.txt
        echo "Commit: ${{ github.sha }}" >> training_summary.txt
        echo "Parameters: n_estimators=${{ github.event.inputs.n_estimators || 100 }}, max_depth=${{ github.event.inputs.max_depth || 10 }}" >> training_summary.txt
        echo "Author: alpian_khairi_C1BO" >> training_summary.txt
        
    - name: Upload workflow summary
      uses: actions/upload-artifact@v4
      with:
        name: workflow-summary-${{ github.run_number }}
        path: training_summary.txt
        retention-days: 30
        if-no-files-found: warn